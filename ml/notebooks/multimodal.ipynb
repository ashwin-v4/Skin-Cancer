{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6785866,"sourceType":"datasetVersion","datasetId":3904493},{"sourceId":7993604,"sourceType":"datasetVersion","datasetId":4706070},{"sourceId":9540389,"sourceType":"datasetVersion","datasetId":5811619},{"sourceId":13019906,"sourceType":"datasetVersion","datasetId":8222001}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport PIL\nimport os\nfrom tqdm import tqdm\nimport copy\nimport shutil\n\nimport torchvision.models as models\nfrom torchvision.transforms import transforms\n\nimport torch\nimport torch.nn as nn \nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:57.906863Z","iopub.execute_input":"2025-09-10T18:33:57.907173Z","iopub.status.idle":"2025-09-10T18:33:57.912453Z","shell.execute_reply.started":"2025-09-10T18:33:57.907143Z","shell.execute_reply":"2025-09-10T18:33:57.911760Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"metadata = pd.read_csv(\"/kaggle/input/metadata/metadata.csv\")\nmetadata = metadata.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:57.913708Z","iopub.execute_input":"2025-09-10T18:33:57.913957Z","iopub.status.idle":"2025-09-10T18:33:57.973347Z","shell.execute_reply.started":"2025-09-10T18:33:57.913936Z","shell.execute_reply":"2025-09-10T18:33:57.972595Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"print(len(metadata))\nprint(metadata.columns)\nprint(metadata.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:57.974145Z","iopub.execute_input":"2025-09-10T18:33:57.974433Z","iopub.status.idle":"2025-09-10T18:33:57.978825Z","shell.execute_reply.started":"2025-09-10T18:33:57.974414Z","shell.execute_reply":"2025-09-10T18:33:57.978187Z"}},"outputs":[{"name":"stdout","text":"18875\nIndex(['binary_label', 'id', 'smoke', 'drink', 'background_father',\n       'background_mother', 'age', 'gender', 'skin_cancer_history',\n       'cancer_history', 'region', 'itch', 'grew', 'hurt', 'changed', 'bleed',\n       'elevation', 'biopsed', 'fitzpatrick'],\n      dtype='object')\n(18875, 19)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"metadata[\"id\"].loc[16572:16579]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:57.980511Z","iopub.execute_input":"2025-09-10T18:33:57.980909Z","iopub.status.idle":"2025-09-10T18:33:57.992537Z","shell.execute_reply.started":"2025-09-10T18:33:57.980888Z","shell.execute_reply":"2025-09-10T18:33:57.992015Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"16572    cd90e491ddaa92f0f4eb07f73aa09f64\n16573    5f1ed6de6a9110d7dc580a6a0312af63\n16574    f198aaf1f0550c2464b285454d34926e\n16575    6214de2e915835014235a1839cbc5938\n16576    5a3a4c1f0effb626b298e89c032b1d28\n16577               PAT_1516_1765_530.png\n16578                  PAT_46_881_939.png\n16579               PAT_1545_1867_547.png\nName: id, dtype: object"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"metadata.binary_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:57.993362Z","iopub.execute_input":"2025-09-10T18:33:57.993598Z","iopub.status.idle":"2025-09-10T18:33:58.005458Z","shell.execute_reply.started":"2025-09-10T18:33:57.993577Z","shell.execute_reply":"2025-09-10T18:33:58.004771Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"binary_label\n0    14793\n1     4082\nName: count, dtype: int64"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"metadata.fitzpatrick.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.006076Z","iopub.execute_input":"2025-09-10T18:33:58.006337Z","iopub.status.idle":"2025-09-10T18:33:58.017155Z","shell.execute_reply.started":"2025-09-10T18:33:58.006315Z","shell.execute_reply":"2025-09-10T18:33:58.016572Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"fitzpatrick\n 2.0    5684\n 3.0    3700\n 1.0    3100\n 4.0    2843\n 5.0    1543\n-1.0    1369\n 6.0     636\nName: count, dtype: int64"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"metadata.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.017925Z","iopub.execute_input":"2025-09-10T18:33:58.018221Z","iopub.status.idle":"2025-09-10T18:33:58.033993Z","shell.execute_reply.started":"2025-09-10T18:33:58.018202Z","shell.execute_reply":"2025-09-10T18:33:58.033498Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 18875 entries, 0 to 18874\nData columns (total 19 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   binary_label         18875 non-null  int64  \n 1   id                   18875 non-null  object \n 2   smoke                18875 non-null  int64  \n 3   drink                18875 non-null  int64  \n 4   background_father    18875 non-null  int64  \n 5   background_mother    18875 non-null  int64  \n 6   age                  18875 non-null  float64\n 7   gender               18875 non-null  int64  \n 8   skin_cancer_history  18875 non-null  int64  \n 9   cancer_history       18875 non-null  int64  \n 10  region               18875 non-null  int64  \n 11  itch                 18875 non-null  int64  \n 12  grew                 18875 non-null  int64  \n 13  hurt                 18875 non-null  int64  \n 14  changed              18875 non-null  int64  \n 15  bleed                18875 non-null  int64  \n 16  elevation            18875 non-null  int64  \n 17  biopsed              18875 non-null  int64  \n 18  fitzpatrick          18875 non-null  float64\ndtypes: float64(2), int64(16), object(1)\nmemory usage: 2.7+ MB\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"pad_base_path = \"/kaggle/input/skin-cancer\"\npad_subfolder_paths = [\n    \"imgs_part_1/imgs_part_1\",\n    \"imgs_part_2/imgs_part_2\",\n    \"imgs_part_3/imgs_part_3\"\n]\npad_set = {}\n\nfor pad_path in pad_subfolder_paths:\n    image_path = os.path.join(pad_base_path, pad_path)\n        \n    for f in os.listdir(image_path):\n        if f.endswith(\".png\"):\n            pad_set[f] = os.path.join(image_path, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.034606Z","iopub.execute_input":"2025-09-10T18:33:58.034852Z","iopub.status.idle":"2025-09-10T18:33:58.049618Z","shell.execute_reply.started":"2025-09-10T18:33:58.034829Z","shell.execute_reply":"2025-09-10T18:33:58.048962Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def resolve_path(image_id):\n    if image_id.endswith(\".png\"):\n        return pad_set.get(image_id, None)\n    else:\n        return image_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.051096Z","iopub.execute_input":"2025-09-10T18:33:58.051694Z","iopub.status.idle":"2025-09-10T18:33:58.054917Z","shell.execute_reply.started":"2025-09-10T18:33:58.051671Z","shell.execute_reply":"2025-09-10T18:33:58.054216Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"metadata[\"full_path\"] = metadata[\"id\"].apply(resolve_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.055629Z","iopub.execute_input":"2025-09-10T18:33:58.055866Z","iopub.status.idle":"2025-09-10T18:33:58.070870Z","shell.execute_reply.started":"2025-09-10T18:33:58.055842Z","shell.execute_reply":"2025-09-10T18:33:58.070323Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"metadata.full_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.090863Z","iopub.execute_input":"2025-09-10T18:33:58.091256Z","iopub.status.idle":"2025-09-10T18:33:58.096526Z","shell.execute_reply.started":"2025-09-10T18:33:58.091215Z","shell.execute_reply":"2025-09-10T18:33:58.095897Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0                         5e82a45bc5d78bd24ae9202d194423f8\n1                         fa2911a9b13b6f8af79cb700937cc14f\n2                         d2bac3c9e4499032ca8e9b07c7d3bc40\n3                         0a94359e7eaacd7178e06b2823777789\n4                         a39ec3b1f22c08a421fa20535e037bba\n                               ...                        \n18870    /kaggle/input/skin-cancer/imgs_part_3/imgs_par...\n18871    /kaggle/input/skin-cancer/imgs_part_1/imgs_par...\n18872    /kaggle/input/skin-cancer/imgs_part_3/imgs_par...\n18873    /kaggle/input/skin-cancer/imgs_part_1/imgs_par...\n18874    /kaggle/input/skin-cancer/imgs_part_3/imgs_par...\nName: full_path, Length: 18875, dtype: object"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"metadata_cols = ['smoke', 'drink', 'background_father',\n       'background_mother', 'age', 'gender', 'skin_cancer_history',\n       'cancer_history', 'region', 'itch', 'grew', 'hurt', 'changed', 'bleed',\n       'elevation', 'biopsed', 'fitzpatrick']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.097839Z","iopub.execute_input":"2025-09-10T18:33:58.098085Z","iopub.status.idle":"2025-09-10T18:33:58.108632Z","shell.execute_reply.started":"2025-09-10T18:33:58.098063Z","shell.execute_reply":"2025-09-10T18:33:58.107973Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class MultimodalSkinCancerDataset(Dataset):\n\n    def __init__(self, df, metadata_cols, transform = None):\n        self.df = df\n        self.transform = transform\n        self.metadata_cols = metadata_cols\n       \n    \n    def __len__(self):\n        return len(self.df)\n\n\n    def __getitem__(self, idx):\n\n        image = None\n        image_path = None\n        fitz_path = \"/kaggle/input/fitzpatrick17k-original/finalfitz17k\"\n\n        row = self.df.iloc[idx]\n        image_id = self.df.iloc[idx][\"id\"]\n        label = self.df.iloc[idx][\"binary_label\"]\n\n        \n        if(image_id.endswith(\".jpg\")):\n            image_path = os.path.join(fitz_path, image_id)\n            image_bgr = cv2.imread(image_path)\n            \n        elif image_id.endswith(\".png\"):  \n            image_path = self.df.iloc[idx][\"full_path\"]\n        \n        else:\n            image_path = os.path.join(fitz_path, image_id + \".jpg\")\n            \n        image_bgr = cv2.imread(image_path)\n\n        if image_bgr is None:\n            image_bgr = np.zeros((224, 224, 3), dtype=np.uint8)\n        \n        image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            image = self.transform(image)\n\n        metadata = row[self.metadata_cols].values.astype(np.float32)\n            \n        return image, metadata, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.109318Z","iopub.execute_input":"2025-09-10T18:33:58.109575Z","iopub.status.idle":"2025-09-10T18:33:58.121139Z","shell.execute_reply.started":"2025-09-10T18:33:58.109554Z","shell.execute_reply":"2025-09-10T18:33:58.120559Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"image_size = 224  \n\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\nval_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.121816Z","iopub.execute_input":"2025-09-10T18:33:58.122095Z","iopub.status.idle":"2025-09-10T18:33:58.135817Z","shell.execute_reply.started":"2025-09-10T18:33:58.122079Z","shell.execute_reply":"2025-09-10T18:33:58.135267Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\ntrain_df, val_df = train_test_split(metadata, test_size=0.2, random_state=42, stratify=metadata['binary_label'])\n\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\ntrain_dataset = MultimodalSkinCancerDataset(df = train_df, metadata_cols=metadata_cols, transform=train_transform)\nval_dataset = MultimodalSkinCancerDataset(df = val_df, metadata_cols=metadata_cols, transform=val_transform)\n\nbatch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.137549Z","iopub.execute_input":"2025-09-10T18:33:58.137777Z","iopub.status.idle":"2025-09-10T18:33:58.160173Z","shell.execute_reply.started":"2025-09-10T18:33:58.137762Z","shell.execute_reply":"2025-09-10T18:33:58.159693Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\n\ntrain_class_counts = train_df['binary_label'].value_counts().to_dict()\ntrain_weights = [1.0 / train_class_counts[label] for label in train_df['binary_label']]\n\nsampler = WeightedRandomSampler(train_weights, num_samples=len(train_weights), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.160931Z","iopub.execute_input":"2025-09-10T18:33:58.161498Z","iopub.status.idle":"2025-09-10T18:33:58.171240Z","shell.execute_reply.started":"2025-09-10T18:33:58.161476Z","shell.execute_reply":"2025-09-10T18:33:58.170505Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    \n    model.train()\n    running_loss = 0\n    \n    for images, metadata, labels in tqdm(loader, desc=\"Training\", leave=True):\n        \n        images = images.to(device)\n        metadata = metadata.to(device)\n        labels = labels.float().unsqueeze(1).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images, metadata)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    return running_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.171986Z","iopub.execute_input":"2025-09-10T18:33:58.172255Z","iopub.status.idle":"2025-09-10T18:33:58.181848Z","shell.execute_reply.started":"2025-09-10T18:33:58.172206Z","shell.execute_reply":"2025-09-10T18:33:58.181213Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, metadata, labels in tqdm(loader, desc=\"Validation\", leave=True):\n            images = images.to(device)\n            metadata = metadata.to(device)\n            labels = labels.float().unsqueeze(1).to(device)\n            \n            outputs = model(images, metadata)  \n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            \n            probs = torch.sigmoid(outputs)  \n            all_preds.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            \n    return running_loss / len(loader), all_preds, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.182455Z","iopub.execute_input":"2025-09-10T18:33:58.182701Z","iopub.status.idle":"2025-09-10T18:33:58.192875Z","shell.execute_reply.started":"2025-09-10T18:33:58.182685Z","shell.execute_reply":"2025-09-10T18:33:58.192187Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def compute_metrics(preds, labels):\n    preds_bin = (np.array(preds) > 0.5).astype(int)\n    labels = np.array(labels)\n    \n    acc = accuracy_score(labels, preds_bin)\n    prec = precision_score(labels, preds_bin)\n    rec = recall_score(labels, preds_bin)\n    f1 = f1_score(labels, preds_bin)\n    auc = roc_auc_score(labels, preds)\n    \n    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': auc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.193455Z","iopub.execute_input":"2025-09-10T18:33:58.193683Z","iopub.status.idle":"2025-09-10T18:33:58.203042Z","shell.execute_reply.started":"2025-09-10T18:33:58.193651Z","shell.execute_reply":"2025-09-10T18:33:58.202547Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"### 1. Resnet18","metadata":{}},{"cell_type":"code","source":"class MultimodalModel(nn.Module):\n    def __init__(self, num_metadata_features, pretrained=True):\n        super().__init__()\n        \n        self.cnn = models.resnet18(pretrained=pretrained)\n        self.cnn.fc = nn.Identity()\n        img_features = 512  \n        \n        self.metadata_fc = nn.Sequential(\n            nn.Linear(num_metadata_features, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(img_features + 32, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)  \n        )\n    \n    def forward(self, image, metadata=None):\n        img_out = self.cnn(image)\n        if metadata is not None:\n            meta_out = self.metadata_fc(metadata)\n            combined = torch.cat([img_out, meta_out], dim=1)\n        else:\n            combined = img_out\n        return self.classifier(combined) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.203937Z","iopub.execute_input":"2025-09-10T18:33:58.204578Z","iopub.status.idle":"2025-09-10T18:33:58.215546Z","shell.execute_reply.started":"2025-09-10T18:33:58.204559Z","shell.execute_reply":"2025-09-10T18:33:58.214906Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = MultimodalModel(num_metadata_features=len(train_dataset.metadata_cols)).to(device)\n\nclass_counts = metadata['binary_label'].value_counts()\nneg_count, pos_count = class_counts[0], class_counts[1]\npos_weight = torch.tensor([neg_count / pos_count], dtype=torch.float32).to(device)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.216213Z","iopub.execute_input":"2025-09-10T18:33:58.216499Z","iopub.status.idle":"2025-09-10T18:33:58.459952Z","shell.execute_reply.started":"2025-09-10T18:33:58.216481Z","shell.execute_reply":"2025-09-10T18:33:58.459426Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"num_epochs = 10\nbest_val_f1 = 0.0  \nbest_val_loss = float('inf') \npatience = 5\nepochs_no_improve = 0\n\nfor epoch in range(num_epochs):\n    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, val_preds, val_labels = validate(model, val_loader, criterion, device)\n    metrics = compute_metrics(val_preds, val_labels)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    print(f\"Metrics: {metrics}\")\n\n    if metrics['f1'] > best_val_f1:\n        best_val_f1 = metrics['f1']\n        torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n        print(\"Model saved! F1-score improved.\")\n        epochs_no_improve = 0 \n    \n    else:\n        epochs_no_improve += 1\n        print(f\"F1-score did not improve for {epochs_no_improve} epochs.\")\n\n    if epochs_no_improve == patience:\n        print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:33:58.460652Z","iopub.execute_input":"2025-09-10T18:33:58.460849Z","iopub.status.idle":"2025-09-10T18:58:22.341454Z","shell.execute_reply.started":"2025-09-10T18:33:58.460833Z","shell.execute_reply":"2025-09-10T18:58:22.340620Z"}},"outputs":[{"name":"stderr","text":"Training:   2%|▏         | 9/472 [00:03<02:40,  2.88it/s][ WARN:0@411.252] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  32%|███▏      | 151/472 [00:42<01:43,  3.09it/s][ WARN:0@449.910] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:07<00:00,  3.71it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 0.6570 | Val Loss: 0.6176\nMetrics: {'accuracy': 0.7952317880794701, 'precision': 0.5153024911032028, 'recall': 0.8872549019607843, 'f1': 0.6519585772174696, 'roc_auc': 0.919701608253981}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/472 [00:00<?, ?it/s][ WARN:0@558.534] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining:  15%|█▌        | 71/472 [00:20<01:32,  4.35it/s][ WARN:0@577.687] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  96%|█████████▌| 451/472 [01:59<00:04,  4.92it/s][ WARN:0@677.832] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:05<00:00,  3.77it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 | Train Loss: 0.4569 | Val Loss: 0.5118\nMetrics: {'accuracy': 0.8770860927152317, 'precision': 0.6651031894934334, 'recall': 0.8688725490196079, 'f1': 0.7534537725823591, 'roc_auc': 0.9431072699441387}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:  87%|████████▋ | 409/472 [01:49<00:11,  5.37it/s][ WARN:0@815.717] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:05<00:00,  3.77it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 | Train Loss: 0.3614 | Val Loss: 0.5536\nMetrics: {'accuracy': 0.8309933774834437, 'precision': 0.5689922480620155, 'recall': 0.8995098039215687, 'f1': 0.6970560303893638, 'roc_auc': 0.9438552372621911}\nF1-score did not improve for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Training:  81%|████████  | 383/472 [01:40<00:22,  4.02it/s][ WARN:0@954.941] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:03<00:00,  3.83it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 | Train Loss: 0.2980 | Val Loss: 0.5521\nMetrics: {'accuracy': 0.8437086092715231, 'precision': 0.5902555910543131, 'recall': 0.9056372549019608, 'f1': 0.7147001934235978, 'roc_auc': 0.9449763599255179}\nF1-score did not improve for 2 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Training:  34%|███▎      | 159/472 [00:42<01:11,  4.37it/s][ WARN:0@1042.466] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  50%|█████     | 236/472 [01:01<00:43,  5.48it/s][ WARN:0@1061.917] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  76%|███████▌  | 358/472 [01:35<00:28,  4.01it/s][ WARN:0@1095.351] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:02<00:00,  3.84it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 | Train Loss: 0.2411 | Val Loss: 0.5570\nMetrics: {'accuracy': 0.8845033112582782, 'precision': 0.6866404715127702, 'recall': 0.8566176470588235, 'f1': 0.7622682660850599, 'roc_auc': 0.9444938671649802}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:  91%|█████████ | 429/472 [01:50<00:12,  3.57it/s][ WARN:0@1256.733] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining:  96%|█████████▋| 455/472 [01:57<00:02,  5.90it/s][ WARN:0@1263.383] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:01<00:00,  3.88it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 | Train Loss: 0.2028 | Val Loss: 0.6711\nMetrics: {'accuracy': 0.8659602649006622, 'precision': 0.6388888888888888, 'recall': 0.8737745098039216, 'f1': 0.7380952380952381, 'roc_auc': 0.9416999648794969}\nF1-score did not improve for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Training:  63%|██████▎   | 297/472 [01:18<00:41,  4.24it/s][ WARN:0@1368.911] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  87%|████████▋ | 412/472 [01:47<00:13,  4.41it/s][ WARN:0@1398.028] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:02<00:00,  3.86it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 | Train Loss: 0.1977 | Val Loss: 0.6559\nMetrics: {'accuracy': 0.898543046357616, 'precision': 0.7267015706806282, 'recall': 0.8504901960784313, 'f1': 0.7837380011293055, 'roc_auc': 0.946316778654686}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|▏         | 7/472 [00:02<02:37,  2.95it/s][ WARN:0@1438.188] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining:  94%|█████████▍| 444/472 [01:55<00:10,  2.78it/s][ WARN:0@1551.018] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:01<00:00,  3.88it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 | Train Loss: 0.1695 | Val Loss: 0.7129\nMetrics: {'accuracy': 0.9149668874172185, 'precision': 0.8136882129277566, 'recall': 0.7867647058823529, 'f1': 0.7999999999999999, 'roc_auc': 0.9468760975157214}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:  61%|██████    | 287/472 [01:12<00:37,  4.87it/s][ WARN:0@1653.288] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/2ea034fc482e9bd21a5dfa2506cc5d6b.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:01<00:00,  3.90it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 | Train Loss: 0.1563 | Val Loss: 0.6101\nMetrics: {'accuracy': 0.909933774834437, 'precision': 0.7644444444444445, 'recall': 0.8431372549019608, 'f1': 0.8018648018648018, 'roc_auc': 0.9551412192115779}\nModel saved! F1-score improved.\n","output_type":"stream"},{"name":"stderr","text":"Training:   6%|▋         | 30/472 [00:09<01:59,  3.70it/s][ WARN:0@1733.770] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  18%|█▊        | 83/472 [00:22<01:12,  5.40it/s][ WARN:0@1747.576] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/e69165b3455bb3a5a8b33a0f6fd8a1d3.jpg'): can't open/read file: check file path/integrity\nTraining:  53%|█████▎    | 252/472 [01:06<00:55,  3.93it/s][ WARN:0@1791.735] global loadsave.cpp:268 findDecoder imread_('/kaggle/input/fitzpatrick17k-original/finalfitz17k/bccab73c32aba48c90602de9bb458272.jpg'): can't open/read file: check file path/integrity\nTraining: 100%|██████████| 472/472 [02:03<00:00,  3.81it/s]\nValidation: 100%|██████████| 118/118 [00:22<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 | Train Loss: 0.1429 | Val Loss: 0.6020\nMetrics: {'accuracy': 0.9101986754966888, 'precision': 0.7647058823529411, 'recall': 0.8443627450980392, 'f1': 0.8025626092020967, 'roc_auc': 0.9567367585763608}\nModel saved! F1-score improved.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_multimodal_model.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T18:58:22.342641Z","iopub.execute_input":"2025-09-10T18:58:22.342972Z","iopub.status.idle":"2025-09-10T18:58:22.401719Z","shell.execute_reply.started":"2025-09-10T18:58:22.342938Z","shell.execute_reply":"2025-09-10T18:58:22.401147Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}